# 3.4 HDFS 的 Shell 操作（二）

## 任务目的

- 掌握查看 HDFS 文本内容的相关命令
- 学会往 HDFS 文件中追加内容和合并下载 HDFS 的多个文件到本地文件系统
- 掌握修改 HDFS 文件权限和拥有者的相关命令
- 掌握统计指定目录和文件系统信息的方法
- 学会使用 HDFS Shell 命令修改文件或目录的副本数

## 任务清单

- 任务 1：查看文本内容命令
- 任务 2：追加和合并文本内容命令
- 任务 3：修改权限命令
- 任务 4：统计命令
- 任务 5：设置副本命令

## 详细任务步骤

### 任务 1：查看文本内容命令

#### 1. cat 命令

　　`cat` 命令**用于将路径指定文件的内容输出到 stdout**，其语法格式如下：

```shell
hadoop fs -cat [-ignoreCrc] <src>
```

　　**示例：**

```shell
hadoop fs -cat /hadoop2.7.7/data/README.txt 
```

　　上述指令执行成功后**会在控制台上打印指定文件的全部内容**，效果如下图所示：

![Vditor](http://assets.qingjiaoclass.com/gdlzpoyzbkrj/20200514/bjveyovr_FeJPFqAKDOVZ4dOpbD69/1589427508uI/_image/1.png)

图1

#### 2. tail 命令

　　`tail`命令**用于将指定文件最后 1K 字节的内容输出到 stdout**，一般**用于查看日志**。其语法格式如下：

```shell
hadoop fs -tail [-f] <file> 
```

　　其中，`-f` 参数用于显示文件增长时附加的数据。

　　**示例：**

```shell
hadoop fs -tail /hadoop2.7.7/data/README.txt 
```

　　上述指令执行成功后会在控制台上**打印指定文件的最后 1K 字节的内容**，效果如下图所示：

![Vditor](http://assets.qingjiaoclass.com/gdlzpoyzbkrj/20200514/bjveyovr_FeJPFqAKDOVZ4dOpbD69/1589427508uI/_image/2.png)

图2

### 任务 2：追加和合并文本内容命令

#### 1. appendToFile 命令

　　`appendToFile` 命令**用于追加一个或多个文件内容到已经存在的文件末尾**，其语法格式如下：

```shell
hadoop fs -appendToFile <localsrc> ... <dst>
```

　　注意：HDFS 文件不能进行修改，但是可以进行追加。

　　**示例：**

　　准备任意 1 个或多个文件（内容随意），将其追加到 README.txt 文件中：

```shell
hadoop fs -appendToFile 1.txt 2.txt /hadoop2.7.7/data/README.txt
```

　　效果如下图所示：

![Vditor](http://assets.qingjiaoclass.com/gdlzpoyzbkrj/20200514/bjveyovr_FeJPFqAKDOVZ4dOpbD69/1589427508uI/_image/3.png)

图3

　　上述指令执行完成，使用 `cat` 命令将文件内容显示到控制台上，效果如下所示：

![Vditor](http://assets.qingjiaoclass.com/gdlzpoyzbkrj/20200514/bjveyovr_FeJPFqAKDOVZ4dOpbD69/1589427508uI/_image/4.png)

图4

#### 2. getmerge 命令

　　`getmerge` 命令**用于合并下载多个文件**，指定一个源目录和一个目标文件，将源目录中所有的文件合并并排序地连接成本地的一个目标文件。其语法格式如下：

```shell
hadoop fs -getmerge [-nl] <src> <localdst>
```

　　其中，`-nl`参数用于在每个文件的末尾添加一个换行符。

　　**示例：**

　　准备任意 3 个文件，分别命名为 1.txt、2.txt 和 3.txt，在这 3 个文件里写入任意内容，这里我分别写入了 1,2,3,将这 3 个文件上传到 HDFS 的/merge（若没有先创建）目录下，创建目录和上传文件的命令如下所示：

```shell
hadoop fs -mkdir /merge
hadoop fs -put 1.txt 2.txt 3.txt /merge
```

　　效果如下图所示：

![Vditor](http://assets.qingjiaoclass.com/gdlzpoyzbkrj/20200514/bjveyovr_FeJPFqAKDOVZ4dOpbD69/1589427508uI/_image/5.png)

图5

　　上传成功后，使用 `getmerge` 命令合并下载这 3 个文件到目标文件 /root/merge.txt 中，命令如下所示：

```shell
hadoop fs -getmerge /merge/*.txt /root/merge.txt
hadoop fs -getmerge -nl /merge/*.txt /root/merge.txt ## ‐nl的作用是在每个文件的末尾添加一个换行符 
```

　　效果如下图所示：

![Vditor](http://assets.qingjiaoclass.com/gdlzpoyzbkrj/20200514/bjveyovr_FeJPFqAKDOVZ4dOpbD69/1589427508uI/_image/6.png)

图6

### 任务 3：修改权限命令

#### 1. chmod 命令

　　`chmod` 命令**用于改变文件的权限**，命令的使用者必须是文件的所有者或者超级用户。其语法格式如下：

```shell
hadoop fs -chmod [-R] PATH
```

　　其中，`-R` 参数将使改变在目录结构下递归进行。

　　**示例：**

```shell
hadoop fs -chmod -R 766 /hadoop2.7.7
```

　　上述指令递归修改了 /hadoop2.7.7 目录下所有文件和文件夹的权限，我们可以使用`ls -R`命令进行验证，效果如下图所示：

![Vditor](http://assets.qingjiaoclass.com/gdlzpoyzbkrj/20200514/bjveyovr_FeJPFqAKDOVZ4dOpbD69/1589427508uI/_image/7.png)

图7

#### 2. chown 命令

　　`chown` 命令**用于改变文件的拥有者或所属组**，命令的使用者必须是文件的所有者或者超级用户。其语法格式如下：

```shell
hadoop fs -chown [-R] [OWNER][:[GROUP]] PATH
```

　　其中，`-R` 参数将使改变在目录结构下递归进行。

　　**示例：**

```shell
hadoop fs -chown hongyaa:hongyaa /hadoop2.7.7/data/README.txt
```

　　上述指令修改了 README.txt 文件的拥有者和所属组，我们可以使用 `ls` 命令进行验证，效果如下图所示：

![Vditor](http://assets.qingjiaoclass.com/gdlzpoyzbkrj/20200514/bjveyovr_FeJPFqAKDOVZ4dOpbD69/1589427508uI/_image/8.png)

图8

### 任务 4：统计命令

#### 1. count 命令

　　`count` 命令**用于统计指定目录下的目录数、文件数和字节数**。其语法格式如下：

```shell
hadoop fs -count [-h] <path>
```

　　其中，`-h` 参数使用便于操作人员读取的单位信息格式。

　　**示例：**

```shell
hadoop fs -count -h /tmp
```

　　上述指令执行成功后会在控制台打印/tmp 目录的各项统计情况，效果如下图所示：

![Vditor](http://assets.qingjiaoclass.com/gdlzpoyzbkrj/20200514/bjveyovr_FeJPFqAKDOVZ4dOpbD69/1589427508uI/_image/9.png)

图9

#### 2. df 命令

　　`df` 命令**用于统计文件系统的容量、可用空间和已用空间信息**。其语法格式如下：

```shell
hadoop fs -df [-h] [<path> ...]
```

　　其中，`-h` 参数使用便于操作人员读取的单位信息格式。

　　**示例：**

```shell
hadoop fs -df -h /
```

　　上述指令执行成功后会在控制台打印文件系统的各项容量信息，效果如下图所示：

![Vditor](http://assets.qingjiaoclass.com/gdlzpoyzbkrj/20200514/bjveyovr_FeJPFqAKDOVZ4dOpbD69/1589427508uI/_image/10.png)

图10

#### 3. du 命令

　　`du` 命令**用于显示指定目录下所有文件和文件夹的大小**，或者当只指定一个文件时，显示此文件的大小。其语法格式如下：

```shell
hadoop fs -du [-s] [-h] <path> 
```

　　其中各项说明如下：

- -s：不显示指定目录下每个单独文件的大小，只统计目录所占用空间的总大小。
- -h：使用便于操作人员读取的单位信息格式。

　　**示例：**

```shell
hadoop fs -du -h /hongya  # 显示指定目录下所有文件和文件夹的大小
hadoop fs -du -s -h /hongya  # 显示指定目录的大小
```

　　为了便于观察，我们可以在根目录下级联创建 hongya/data 目录，将本地文件系统的`$HADOOP_HOME/share/hadoop/mapreduce` 目录下的 hadoop-mapreduce-examples-2.7.7.jar 代码包上传到 /hongya/data 目录中，将 `$HADOOP_HOME` 目录下的 README.txt 和 NOTICE.txt 文件上传到 /hongya 目录下，之后执行上述命令进行查看，效果如下图所示：

![Vditor](http://assets.qingjiaoclass.com/gdlzpoyzbkrj/20200514/bjveyovr_FeJPFqAKDOVZ4dOpbD69/1589427508uI/_image/11.png)

图11

### 任务 5：设置副本命令

　　`setrep` 命令**用于改变 HDFS 中文件的副本系数**。其语法格式如下：

```shell
hadoop fs -setrep [-R] <rep> <path>
```

　　其中，`-R` 参数用于递归改变指定目录下所有文件的副本系数。

　　**示例：**

```shell
hadoop fs -setrep 3 /hongya/README.txt
```

　　上述执行成功后，使用 `ls` 命令进行验证，发现 README.txt 文件的副本数成功修改为 3 份，效果如下图所示：

![Vditor](http://assets.qingjiaoclass.com/gdlzpoyzbkrj/20200514/bjveyovr_FeJPFqAKDOVZ4dOpbD69/1589427508uI/_image/12.png)

图12

　　注意：由于我们搭建的 Hadoop 伪分布式集群只有 1 个 DataNode，且每个 DataNode 上最多只有 1 个副本，所以即使我们手动设置了该文件的副本数为 3，其还是只有 1 个副本。

　　我们可以通过 HDFS 的 Web UI 界面（http://localhost:50070）查看 /hongya/README.txt 文件的副本所在位置：

![Vditor](http://assets.qingjiaoclass.com/gdlzpoyzbkrj/20200514/bjveyovr_FeJPFqAKDOVZ4dOpbD69/1589427508uI/_image/13.png)

图13

　　该副本位于本机上：

![Vditor](http://assets.qingjiaoclass.com/gdlzpoyzbkrj/20200514/bjveyovr_FeJPFqAKDOVZ4dOpbD69/1589427508uI/_image/14.png)

图14